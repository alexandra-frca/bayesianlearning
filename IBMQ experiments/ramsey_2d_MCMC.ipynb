{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ramsey_2d_MCMC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtlcfnNvysnG",
        "outputId": "a7b317aa-2b27-423d-fc8a-997ed39920b5"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Hamiltonian learning implementation for estimating a frequency and a dephasing\n",
        "factor, using offline Bayesian inference.\n",
        "The qubit is assumed to be initialized at state |1> for each iteration, and to\n",
        "evolve under H = f*sigma_x/2, apart from the exponential decay resulting from \n",
        "the presence of decoherence. Estimation is performed for both the precession\n",
        "frequency and the coherence time (or its inverse).\n",
        "A sequential Monte Carlo approximation is used to represent the probability \n",
        "distributions, using Hamiltonian Monte Carlo and/or Metropolis-Hastings mutation \n",
        "steps.\n",
        "The evolution of the standard deviations with the steps is plotted, and the \n",
        "final  values of these quantities (in addition to the actual error) are \n",
        "printed.\n",
        "\"\"\"\n",
        "\n",
        "import sys, copy, random, pickle, itertools, matplotlib.pyplot as plt\n",
        "from autograd import grad, numpy as np\n",
        "np.seterr(all='warn')\n",
        "dim=2\n",
        "total_MH, accepted_MH = 0, 0\n",
        "\n",
        "N_particles = 1 # Number of samples used to represent the probability\n",
        "#distribution, using a sequential Monte Carlo approximation.\n",
        "\n",
        "f_real, alpha_real = 0, 0 \n",
        "\n",
        "left_boundaries = np.array([0,1/25])\n",
        "right_boundaries = np.array([5,1/5])\n",
        "\n",
        "def measure(t, particle=np.array([f_real,alpha_real])):\n",
        "    '''\n",
        "    Simulates the measurement of the quantum system of the z component of spin \n",
        "    at a given time t.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    t: float\n",
        "        The evolution time between the initialization and the projection.\n",
        "    particle: [float], optional\n",
        "        The set of real dynamical parameters governing the system's evolution \n",
        "    (Default is [f_real,alpha_real]).\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    1 if the result is |1>, 0 if it is |0>.\n",
        "    '''\n",
        "    r = np.random.binomial(1, \n",
        "                           p=(np.cos(2*np.pi*f_real*t/2)**2*np.exp(-alpha_real*t)+\n",
        "                              (1-np.exp(-alpha_real*t))/2))\n",
        "    return r\n",
        "\n",
        "def simulate_1(particle, t):\n",
        "    '''\n",
        "    Provides an estimate for the likelihood  P(D=1|test_f,t) of an x-spin \n",
        "    measurement at time t yielding result |+>, given a set of parameters for \n",
        "    the fixed form Hamiltonian. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    particle: [float]\n",
        "        The set of dynamical parameters to be used in the simulation.\n",
        "    t: float\n",
        "        The evolution time between the initialization and the projection.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    p: float\n",
        "        The estimated probability of finding the particle at state |1>.\n",
        "    '''\n",
        "    test_f, test_alpha = particle\n",
        "    p=np.cos(2*np.pi*test_f*t/2)**2*np.exp(-test_alpha*t)+\\\n",
        "        (1-np.exp(-test_alpha*t))/2\n",
        "    return p \n",
        "\n",
        "def likelihood(data,particle):\n",
        "    '''\n",
        "    Provides an estimate for the likelihood  P(D|test_f,t) of z-spin \n",
        "    measurements yielding the input vector of data, given test parameters for  \n",
        "    the fixed form Hamiltonian. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: [(float,int)]\n",
        "        A vector of experimental results obtained so far and their respective \n",
        "        controls, each datum being of the form (time,outcome), where 'time' is          \n",
        "        the control used for each experiment and 'outcome' is its result.\n",
        "    particle: [float]\n",
        "        The set of dynamical parameters to be used for the likelihood.s\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    p: float\n",
        "        The estimated probability of obtaining the input outcome. \n",
        "    '''\n",
        "    if np.size(data)==2: # Single datum case.\n",
        "        t,outcome = data if len(data)==2 else data[0] # May be wrapped in array.\n",
        "        p = simulate_1(particle,t) if outcome==1 else (1-simulate_1(particle,t))\n",
        "    else:\n",
        "        p = np.product([likelihood(datum, particle) for datum in data])\n",
        "    return p \n",
        "\n",
        "first_metropolis_hastings_step = True\n",
        "def metropolis_hastings_step(data, particle, M,\n",
        "                             factor=0.001,\n",
        "                             left_constraints=left_boundaries, \n",
        "                             right_constraints=right_boundaries):\n",
        "    '''\n",
        "    Performs a Metropolis-Hastings mutation on a given particle, using a \n",
        "    gaussian function for the proposals.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: [(float,int)]\n",
        "        A vector of experimental results obtained so far and their respective \n",
        "        controls, each datum being of the form (time,outcome), where 'time' is          \n",
        "        the control used for each experiment and 'outcome' is its result.\n",
        "    particle: [float]\n",
        "        The particle to undergo a mutation step.\n",
        "    S: [[float]]\n",
        "        The covariance matrix that will determine the standard deviations\n",
        "        of the normal distributions used for the proposal in each dimension\n",
        "        (the dth diagonal element corresponding to dimension d).\n",
        "    factor: float\n",
        "        The factor M is to be multiplied by to get the actual standard \n",
        "        deviations.\n",
        "    left_constraints: [float]\n",
        "        The leftmost bounds to be enforced for the particle's motion.\n",
        "    right_constraints: [float]\n",
        "        The rightmost bounds to be enforced for the particle's motion.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    particle: [float]\n",
        "        The mutated particle.\n",
        "    p: float\n",
        "        The acceptance probability to be used for the evolved particle as a \n",
        "        Monte Carlo proposal.\n",
        "    '''\n",
        "    Cov = M*factor\n",
        "    global first_metropolis_hastings_step\n",
        "    if first_metropolis_hastings_step:\n",
        "      print(\"MH:  Cov = \", Cov)\n",
        "      first_metropolis_hastings_step = False\n",
        "\n",
        "    global accepted_MH, total_MH\n",
        "    total_MH += 1\n",
        "\n",
        "    # Start with any invalid point.\n",
        "    new_particle = np.array([left_constraints[i]-1 for i in range(dim)]) \n",
        "    \n",
        "    # Get a proposal that satisfies the constraints.\n",
        "    while any([new_particle[i]<left_constraints[i] for i in range(dim)] + \n",
        "                  [new_particle[i]>right_constraints[i] for i in range(dim)]):\n",
        "        # new_particle = np.array([np.random.normal(particle[i], Sigma[i][i])\n",
        "        #                          for i in range(dim)])\n",
        "        new_particle = np.random.multivariate_normal(particle, Cov)\n",
        "        #print(\"Proposal: \", new_particle)\n",
        "\n",
        "    p = likelihood(data,new_particle)/likelihood(data,particle)\n",
        "    #print(\"p=\", p)\n",
        "    a = min(1,p)\n",
        "    if (np.random.rand() < a):\n",
        "        accepted_MH += 1\n",
        "        return(new_particle)\n",
        "    else:\n",
        "        return(particle)\n",
        "\n",
        "resampler_calls = 0\n",
        "first_bayes_update = True\n",
        "def bayes_update(data, particle_list, moves=10000):\n",
        "    '''\n",
        "    Updates a prior distribution according to the outcome of a measurement, \n",
        "    using Bayes' rule. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: [(float,int)]\n",
        "        A vector of experimental results obtained so far and their respective \n",
        "        controls, each datum being of the form (time,outcome), where 'time' is          \n",
        "        the control used for each experiment and 'outcome' is its result.\n",
        "    distribution: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle=[f,alpha]:=[frequency,decay factor] (as a bit string)\n",
        "        The prior distribution (SMC approximation). \n",
        "\n",
        "    Returns\n",
        "    ------- \n",
        "    distribution: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle=[f,alpha]:=[frequency,decay factor] (as a bit string)\n",
        "        The updated distribution (SMC approximation).\n",
        "    resampled: bool\n",
        "        Whether resampling has occurred.\n",
        "    '''\n",
        "    global first_bayes_update\n",
        "    if first_bayes_update:\n",
        "        print(\"Performing %d Markov move(s) per\"\n",
        "        \" call. [bayes_update]\" % (moves))\n",
        "        first_bayes_update = False\n",
        "\n",
        "    global resampler_calls\n",
        "    resampler_calls += moves\n",
        "\n",
        "    new_particle_list = []\n",
        "    for particle in particle_list:\n",
        "      for m in range(moves):\n",
        "        particle = metropolis_hastings_step(data,particle,np.diag([1,right_boundaries[1]/right_boundaries[0]])**2)\n",
        "      new_particle_list.append(particle)\n",
        "\n",
        "    print(new_particle_list)\n",
        "    return new_particle_list\n",
        "\n",
        "def SMCparameters(particle_list, stdev=True, list=False):\n",
        "    '''\n",
        "    Calculates the mean and (optionally) standard deviation of a given \n",
        "    distribution.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    distribution: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle=[f,alpha]:=[frequency,decay factor] (as a bit string)\n",
        "        The distribution (SMC approximation) whose parameters are to be \n",
        "        calculated. This can also be a list if the weights are uniform.\n",
        "    stdev: bool\n",
        "        To be set to False if the standard deviation is not to be returned \n",
        "        (Default is True).\n",
        "    list: bool, optional\n",
        "        To be set to True if the distribution is given as a list (as opposed to\n",
        "        a dictionary) (weights are then assumed even) (Default is False).\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    means: [float]\n",
        "        The means of the distribution along its two dimensions: frequency and\n",
        "        decay factor (the inverse of the coherence time), by this order.\n",
        "    stdevs: [float]\n",
        "        The standard deviation of the distribution along its two dimensions: \n",
        "        frequency and decay factor (the inverse of the coherence time), by this \n",
        "        order.\n",
        "    '''\n",
        "    means, meansquares = np.zeros(dim),np.zeros(dim)\n",
        "    # Distribution is given as a dictionary with implicitly uniform weights.\n",
        "    w = 1/len(particle_list)\n",
        "    for particle in particle_list:\n",
        "        means += particle*w\n",
        "        meansquares += particle**2*w\n",
        "    if not stdev:\n",
        "        return means\n",
        "    stdevs = abs(means**2-meansquares)**0.5\n",
        "    return means,stdevs\n",
        "\n",
        "def meansquarederror(particle_list, real_parameters):\n",
        "    '''\n",
        "    Calculates the mean squared error given an SMC distribution.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    distribution: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle=[f,alpha]:=[frequency,decay factor] (as a bit string)\n",
        "        The distribution (SMC approximation) whose parameters are to be \n",
        "        calculated. This can also be a list if the weights are uniform.\n",
        "    real_parameters: [float]\n",
        "        The set of real parameters.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    r: [float]\n",
        "        The list of mean squared errors along each dimension (for each \n",
        "        parameter).\n",
        "    '''\n",
        "    global dim, N_particles\n",
        "    r = np.zeros(dim)\n",
        "    w = 1/len(particle_list)\n",
        "    for particle in particle_list:\n",
        "        r += (particle-real_parameters)**2*w\n",
        "    return r\n",
        "\n",
        "def plot_distribution(particle_list, note=\"\"):\n",
        "    '''\n",
        "    Plots a discrete distribution, by scattering points as circles with \n",
        "    diameters proportional to their weights. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    distribution: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle the parameter vector (as a bit string)\n",
        "        The distribution to be plotted (SMC approximation).\n",
        "    note: str, optional\n",
        "        Some string to be appended to the graph title (Default is \"\"). \n",
        "    '''\n",
        "    lbound, rbound = left_boundaries, right_boundaries\n",
        "    \n",
        "    fig, axs = plt.subplots(1,figsize=(8,8))\n",
        "    i=0\n",
        "\n",
        "    plt.xlim([lbound[i],rbound[i]])\n",
        "    plt.ylim([lbound[2*i+1],rbound[2*i+1]])\n",
        "    \n",
        "    plt.title(\"Dimensions %d and %d %s\" % (2*i+1,2*i+2,note))\n",
        "    plt.xlabel(\"Parameter number %d\" % (2*i+1))\n",
        "    plt.ylabel(\"Parameter number %d\" % (2*i+2))\n",
        "    \n",
        "    x1 = [particle[i] for particle in particle_list]\n",
        "    x2 = [particle[i+1] for particle in particle_list]\n",
        "    weight = 200/N_particles\n",
        "    axs.scatter(x1, x2, marker='o',s=weight)\n",
        "\n",
        "def show_results(off_runs,off_dicts,fs,alphas,parameters):\n",
        "    '''\n",
        "    Processes results and plots and prints statistical quantities of interest. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    off_runs: ([float],[float])\n",
        "        A tuple containing:\n",
        "        - A list of length-2 lists of means [mean_f,mean_alpha]);\n",
        "        - A list of length-2 lists of standard deviations [std_f,std_alpha];\n",
        "        Both by increasing iteration order.\n",
        "    off_dicts: [dict]\n",
        "        A list of final distributions respecting different runs.\n",
        "    fs: [float]\n",
        "        A list of final frequency means. Could be gotten from 'off_runs' or \n",
        "        'off_dicts', but this makes it easier.\n",
        "    alphas: [float]\n",
        "        A list of final dephasing factor means. Same as 'fs'.\n",
        "    parameters: [float]\n",
        "        The list of real parameters, for computing the mean squared errors. Will\n",
        "        be an approximation if they're not exact (but based on IBM backend \n",
        "        properties) and the data is imported.\n",
        "    '''\n",
        "    #####\n",
        "    '''\n",
        "    The indexes in off_runs are, by order: \n",
        "        - Run number;\n",
        "        - Desired quantity (0 for mean, 1 for stdev, 2 for cumulative_time)\n",
        "        - Step number\n",
        "        - Desired parameter (0 for frequency, 1 for alpha)\n",
        "    '''\n",
        "    runs = len(off_runs)\n",
        "    steps = len(off_runs[0][0])-1\n",
        "\n",
        "    '''\n",
        "    # Get run with the median last step variance to print its mean squared error\n",
        "    #instead of calculating all of them since we won't plot them.\n",
        "    fstdevs = [s[steps-1][0] for m,s,t in off_runs]\n",
        "    median = np.percentile(fstdevs, 50, interpolation='nearest')\n",
        "    median_index = fstdevs.index(median)\n",
        "    median_dist = off_dicts[median_index]\n",
        "    mses = meansquarederror(median_dist,parameters[median_index])'''\n",
        "\n",
        "    # Get run with the median last step frequency to plot.\n",
        "    #fstdevs = [s[steps-1][0] for m,s,t in off_runs]\n",
        "    median = np.nanpercentile(fs, 50, interpolation='nearest')\n",
        "    median_index = fs.index(median)\n",
        "    median_dist = off_dicts[median_index]\n",
        "    plot_distribution(median_dist,note=\" (run with median frequency)\")\n",
        "    # Same for decay factor.\n",
        "    median = np.nanpercentile(alphas, 50, interpolation='nearest')\n",
        "    median_index = alphas.index(median)\n",
        "    median_dist = off_dicts[median_index]\n",
        "    plot_distribution(median_dist,note=\" (run with median α)\")\n",
        "\n",
        "    mses = meansquarederror(median_dist,parameters[median_index])\n",
        "    all_mses = [meansquarederror(dist,parameters[i]) \\\n",
        "            for i,dist in enumerate(off_dicts)]\n",
        "    median_mses = [np.median([mse[d] for mse in all_mses]) for d in range(dim)]\n",
        "\n",
        "    off_errors = [[abs(off_runs[i][0][steps-1][p]-parameters[i][p]) \n",
        "                  for i in range(runs)] for p in range(2)]  \n",
        "\n",
        "    off_error,off_stdevs,off_precisions_all,off_precisions, off_stdevs_q1s,\\\n",
        "        off_stdevs_q3s = [[],[]], [[],[]], [[],[]], [[],[]], [[],[]], [[],[]]\n",
        "    \n",
        "    for p in range(dim):\n",
        "        off_error[p] = np.median(off_errors)\n",
        "        off_stdevs[p] = [np.median([s[i][p] for m,s,t in off_runs]) \\\n",
        "                      for i in range(steps+1)]\n",
        "        off_stdevs_q1s[p] = [np.percentile([s[i][p] \n",
        "                                            for m,s,t in off_runs], 25) \n",
        "                            for i in range(steps+1)]\n",
        "        off_stdevs_q3s[p] = [np.percentile([s[i][p] for m,s,t in off_runs], 75) \n",
        "                            for i in range(steps+1)]\n",
        "    \n",
        "    median_f = np.median(fs)\n",
        "    median_alpha = np.median(alphas)\n",
        "    print(\"----------------\")\n",
        "    print(\"Median results: \")\n",
        "    print(\"- f =        %.4f   ± %.4f\" % (median_f,off_stdevs[0][steps]))\n",
        "    print(\"- alpha =    %.6f ± %.6f (T2* = %.2f ± %.2f)\" % \n",
        "          (median_alpha, off_stdevs[1][steps], \n",
        "           1/median_alpha, off_stdevs[1][steps]/median_alpha**2))\n",
        "    print(\"- Variance:  %.6f; %.8f\\n- MSE:       %.6f, %.8f\\n\"\n",
        "    \"- Deviation: %.4f; %.6f\"       % (off_stdevs[0][steps]**2, \n",
        "                                      off_stdevs[1][steps]**2, \n",
        "                                      median_mses[0], median_mses[1],\n",
        "                                      off_error[0],\n",
        "                                      off_error[1]))\n",
        "    \n",
        "    fig, axs = plt.subplots(2,figsize=(8,8))\n",
        "    fig.subplots_adjust(hspace=0.55)\n",
        "\n",
        "    p=0\n",
        "    x1 = np.array([i for i in range(steps+1)])\n",
        "    oy1 = np.array([off_stdevs[p][i] for i in range(steps+1)])\n",
        "    axs[0].set_ylabel(r'$\\sigma$')\n",
        "    axs[0].plot(x1, oy1, color='red', label='offline estimation')\n",
        "    oq11 = np.array([off_stdevs_q1s[p][i] for i in range(steps+1)])\n",
        "    oq31 = np.array([off_stdevs_q3s[p][i] for i in range(steps+1)])\n",
        "    axs[0].fill_between(x1,oq11,oq31,alpha=0.1,color='red')\n",
        "    axs[0].set_title('Frequency Estimation')\n",
        "    axs[0].set_xlabel('Iteration number')\n",
        "    axs[0].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    \n",
        "    p=1\n",
        "    oy1 = np.array([off_stdevs[p][i] for i in range(steps+1)])\n",
        "    axs[1].set_ylabel(r'$\\sigma$')\n",
        "    axs[1].plot(x1, oy1, color='red', label='offline estimation')\n",
        "    oq11 = np.array([off_stdevs_q1s[p][i] for i in range(steps+1)])\n",
        "    oq31 = np.array([off_stdevs_q3s[p][i] for i in range(steps+1)])\n",
        "    axs[1].fill_between(x1,oq11,oq31,alpha=0.1,color='red')\n",
        "    axs[1].set_title('Coherence Factor Estimation')\n",
        "    axs[1].set_xlabel('Iteration number')\n",
        "    axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    \n",
        "def plot_grid(data, particle_list, note=\"\"):\n",
        "    '''\n",
        "    Plots a discrete distribution, by scattering points as circles with \n",
        "    diameters proportional to their weights. Also signalizes the target modes.\n",
        "    A graph will be produced for each pair of dimensions.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: [(float,int)]\n",
        "        A vector of (evolution time, outcome) tuples.\n",
        "    distribution: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle the parameter vector (as a bit string)\n",
        "        The distribution to be plotted (SMC approximation).\n",
        "    note: str, optional\n",
        "        Some string to be appended to the graph title (Default is \"\"). \n",
        "    '''\n",
        "    lbound, rbound = left_boundaries, right_boundaries\n",
        "    \n",
        "    fig, axs = plt.subplots(1,figsize=(8,8))\n",
        "    i=0\n",
        "\n",
        "    plt.xlim([lbound[i],rbound[i]])\n",
        "    plt.ylim([lbound[2*i+1],rbound[2*i+1]])\n",
        "    \n",
        "    plt.title(\"Dimensions %d and %d %s\" % (2*i+1,2*i+2,note))\n",
        "    plt.xlabel(\"Parameter number %d\" % (2*i+1))\n",
        "    plt.ylabel(\"Parameter number %d\" % (2*i+2))\n",
        "    \n",
        "    x1 = [particle[i] for particle in particle_list]\n",
        "    x2 = [particle[i+1] for particle in particle_list]\n",
        "    weights = [likelihood(data,particle)*500 \\\n",
        "               for particle in particle_list]\n",
        "    axs.scatter(x1, x2, marker='o',s=weights)\n",
        "\n",
        "def test_on_grid(data, side):\n",
        "    '''\n",
        "    Plots a grid of points with diameters proportional to their likelihoods \n",
        "    (given some data vector).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data: [(float,int)]\n",
        "        A vector of (evolution time, outcome) tuples.\n",
        "    side: int, optional\n",
        "        The side of the 'grid' of particles to be considered.\n",
        "    '''\n",
        "\n",
        "    global f_real, alpha_real, N_particles\n",
        "    #f_real, alpha_real = 1.83, 1/12\n",
        "    N_particles = side**dim\n",
        "    grid = uniform_prior()\n",
        "\n",
        "    plot_grid(data, grid, note=\" (grid likelihood plot)\")\n",
        "    \n",
        "def print_stats(runs,steps,acceptance_only=False):\n",
        "    '''\n",
        "    Prints some numbers related to the resampler and settings.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    runs: int\n",
        "        The total number of runs.\n",
        "    steps: int\n",
        "        The total number of iterations of each run.\n",
        "    '''\n",
        "    if not acceptance_only:\n",
        "        print(\"* Number of resampler calls per run: %d\" \n",
        "              % (resampler_calls/runs))\n",
        "    if (total_MH != 0):\n",
        "        print(\"* Metropolis-Hastings:     %d%% mean particle acceptance rate.\" \n",
        "           % round(100*accepted_MH/total_MH))\n",
        "    print(\"(n=%d; runs=%d; 2d)\" % (N_particles,runs))\n",
        "\n",
        "def uniform_prior():\n",
        "    '''\n",
        "    Generates a flat prior distribution within the defined boundaries.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    prior: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle=[f,alpha]:=[frequency,decay factor] (as a bit string)\n",
        "        The prior distribution (SMC approximation).\n",
        "\n",
        "    '''\n",
        "    f_min, alpha_min = left_boundaries\n",
        "    f_max, alpha_max = right_boundaries\n",
        "    each = int(round(N_particles**(1/dim)))\n",
        "    fs = np.linspace(f_min,f_max,each)\n",
        "    alphas = np.linspace(alpha_min,alpha_max,each)\n",
        "    particle_list = list(itertools.product(fs, alphas))\n",
        "    particle_list = [np.array(particle) for particle in particle_list]\n",
        "    return particle_list\n",
        "\n",
        "def offline_estimation(particle_list, data, steps, plot=False):\n",
        "    '''\n",
        "    Estimates the precession frequency by defining a set of experiments, \n",
        "    performing them, and updating a given prior distribution according to their \n",
        "    outcomes (using Bayesian inference).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    distribution: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle=[f,alpha]:=[frequency,decay factor] (as a bit string)\n",
        "        The prior distribution (SMC approximation).\n",
        "    data: [(float,int)]\n",
        "        A vector of (evolution time, outcome) tuples.\n",
        "    plot: bool, optional\n",
        "        Whether to plot the distribution a few times throughout (Default is \n",
        "        False).\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    distribution: dict\n",
        "        , with (key,value):=(particle,importance weight) \n",
        "        , and particle=[f,alpha]:=[frequency,decay factor] (as a bit string)\n",
        "        The posterior distribution (SMC approximation).\n",
        "    means: [float]\n",
        "        A list of the consecutive distribution means, including the prior's and\n",
        "        the ones resulting from every intermediate step.\n",
        "    stdevs: [float]\n",
        "        A list of the consecutive distribution standard deviations, including \n",
        "        the prior's and the ones resulting from every intermediate step.\n",
        "    _: str\n",
        "        A placeholder for compatibility with how 'show_results()' was originally\n",
        "        structured.\n",
        "    ''' \n",
        "    current_mean, current_stdev = SMCparameters(particle_list)\n",
        "    means, stdevs = [], []\n",
        "    means.append(current_mean)\n",
        "    stdevs.append(current_stdev) \n",
        "\n",
        "    updates = steps\n",
        "    if updates < 10:\n",
        "        progress_interval = 100/updates\n",
        "    print(\"|0%\",end=\"|\"); counter = 0\n",
        "    resampler_calls = 0\n",
        "    for i in range(updates):\n",
        "        if plot and i%(updates/10)==0: # Generate 10 distribution plots.\n",
        "            info = \"- step %d\" % (i)\n",
        "            resampler_calls = 0\n",
        "            plot_distribution(particle_list,note=info)\n",
        "\n",
        "        # Update the distribution: get the posterior of the current iteration, \n",
        "        #which is the prior for the next.\n",
        "        chunksize = len(data)//updates\n",
        "        curr_data = data[0:((i+1)*chunksize)]\n",
        "        particle_list = bayes_update(curr_data, particle_list) \n",
        "\n",
        "        current_mean, current_stdev = SMCparameters(particle_list)\n",
        "        means.append(current_mean)\n",
        "        stdevs.append(current_stdev) \n",
        "\n",
        "        if updates < 10:\n",
        "            counter+=progress_interval\n",
        "            print(round(counter),\"%\",sep=\"\",end=\"|\")\n",
        "        elif (i%(updates/10)<1): \n",
        "            counter+=10\n",
        "            print(counter,\"%\",sep=\"\",end=\"|\")\n",
        "    print(\"\")\n",
        "    if plot:\n",
        "        plot_distribution(particle_list,note=\" (final distribution)\")\n",
        "    return particle_list, (means, stdevs, _)\n",
        "\n",
        "def get_data(upload=False, filename=None, every=1, steps=75, rep=1, rev=False,\n",
        "             tmin=0.02, tmax=3.5, rand=False):\n",
        "    '''\n",
        "    Provides a data vector for the inference, either by loading it from a file \n",
        "    or by generating it.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    upload: bool, optional\n",
        "        Whether to load the data stored in a file (Default is False).\n",
        "    filename: str, optional\n",
        "        The name of the file from where the data should be loaded if 'upload' is\n",
        "        True (Default is None).\n",
        "    steps: int, optional\n",
        "        The number of measurements to perform if upload is False (Default is \n",
        "        75).\n",
        "    tmax: float, optional\n",
        "        The maximum evolution time to be used if upload is False (Default is \n",
        "        3.5).\n",
        "    rand: bool, optional\n",
        "        Whether to choose the evolution times at random from [0,tmax[ if upload \n",
        "        is False (Default us False, the times will be spaced evenly instead).\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    data: [(float,int)]\n",
        "        A vector of (evolution time, outcome) tuples.\n",
        "    '''\n",
        "    if upload:\n",
        "        print(\"> Uploading data from file \\'%s\\'...\" % filename)\n",
        "        with open(filename, 'rb') as filehandle: \n",
        "            data = pickle.load(filehandle)\n",
        "            # Overwrite step number and maximum time.\n",
        "            steps, tmax = len(data), max([t for t,outcome in data])\n",
        "            # Flip the outcomes because the code is structured oppositely to\n",
        "            #the IBM experiments.\n",
        "            data = [(t,outcome^1) for t,outcome in data]\n",
        "            data = data[::every]\n",
        "            if rev:\n",
        "                data = data[::-1]\n",
        "            steps, tmin, tmax = len(data), min([t for t,outcome in data]), \\\n",
        "                max([t for t,outcome in data])\n",
        "    else:\n",
        "        if rand:\n",
        "            ts = [random.uniform(tmin,tmax) for i in range(steps)]\n",
        "        else:\n",
        "            ts = np.linspace(tmin,tmax,steps)\n",
        "            ts = np.repeat(ts,rep)\n",
        "            if rev:\n",
        "                ts = ts[::-1]\n",
        "        print((\"> Using randomly generated \" if rand else \n",
        "              \"> Using evenly spaced ordered \") + \"times.\")\n",
        "        data = [(t,measure(t)) for t in ts]\n",
        "    print(\"Data vector:\", data)\n",
        "    print(\"> t∈[%.1f,%.1f[; steps = %d; f∈[%.1f,%.1f[; α∈[%.2f,%.2f[ (T2*∈[%.2f,%.2f[)\" \n",
        "           % (tmin,tmax,steps,left_boundaries[0],right_boundaries[0],\n",
        "              left_boundaries[1], right_boundaries[1],\n",
        "              1/right_boundaries[1],1/left_boundaries[1]))\n",
        "    return data,steps\n",
        "\n",
        "def main():\n",
        "    global f_real, alpha_real, N_particles, right_boundaries, dim\n",
        "    f_max, alpha_max = right_boundaries\n",
        "    f_real, alpha_real = 1.83, 1/13\n",
        "    print(\"> f = %.2f, alpha = %.2f (T2* = %.2f)\" % \n",
        "          (f_real,alpha_real,1/alpha_real))\n",
        "    \n",
        "    gaps = right_boundaries - left_boundaries\n",
        "    center = [left_boundaries[d] + gaps[d]/2 for d in range(dim)]\n",
        "    cov = np.diag([0,0]) #np.diag([1,0.1]) #np.identity(dim) #np.diag(gaps/4)**2\n",
        "\n",
        "    particle_list = []\n",
        "    for i in range(N_particles):\n",
        "      particle = np.array([left_boundaries[i]-1 for i in range(dim)]) \n",
        "      # Get a particle that satisfies the constraints.\n",
        "      while any([particle[i]<left_boundaries[i] for i in range(dim)] + \n",
        "                    [particle[i]>right_boundaries[i] for i in range(dim)]):\n",
        "          particle = np.random.multivariate_normal(center, cov)\n",
        "          print(\"Particle:\",particle)\n",
        "      particle_list.append(particle)\n",
        "\n",
        "    print(particle_list)\n",
        "    steps = 3\n",
        "    \n",
        "    datasets = 1\n",
        "    runs_each = 1\n",
        "    print(\"> Will use %d datasets for %d runs each to compute the statistics.\" \n",
        "          % (datasets,runs_each))\n",
        "    runs = datasets*runs_each\n",
        "    off_runs = []\n",
        "    parameters = [np.array([f_real,alpha_real]) for i in range(runs)]\n",
        "    fs, alphas = [], []\n",
        "    off_dicts = []\n",
        "    try:\n",
        "          for i in range(datasets):\n",
        "              print(f\"> Dataset {i}.\")\n",
        "\n",
        "              filename = 'guadalupe_ramsey_data[2.0,5[f=2_sched=75_nshots=2_' + str(i) + '.data'\n",
        "              '''\n",
        "              filename = 'ramsey_data[0.2,5[df=1.83_sched=75_nshots=1_' + str(i) + '.data' if i<5 \\\n",
        "                  else 'ramsey_data[0.2,5[df=1.83_sched=75_nshots=5_' + str(i%5) + '.data'\n",
        "              every = 1 if i<5 else 5\n",
        "              '''\n",
        "              data,meas = get_data(filename=filename, every=1, upload=False, \n",
        "                                 steps=75, rep=2, rev=False, tmin=0.2, tmax=5, rand=False)\n",
        "\n",
        "              for j in range(runs_each):\n",
        "                  (dist,sequences) = offline_estimation(copy.deepcopy(particle_list),\n",
        "                                                        data,steps, plot=False)\n",
        "                  off_runs.append(sequences)\n",
        "                  off_dicts.append(dist)\n",
        "                  f,alpha = sequences[0][steps]\n",
        "                  fs.append(f)\n",
        "                  alphas.append(alpha)\n",
        "                  print(\"> Estimated f=%.2f and alpha=%.2f (T2=%.2f).\"\n",
        "                      % (f,alpha,1/alpha))\n",
        "    except KeyboardInterrupt:\n",
        "          err = sys.exc_info()[0]\n",
        "          runs = len(fs)\n",
        "          print(\"> Quit at run %d (%s).\" % (runs,err))\n",
        "          if runs!=0:\n",
        "            print_stats(runs,steps)\n",
        "          else:\n",
        "            print(\"Incomplete first run statistics:\")\n",
        "            print_stats(1, steps, acceptance_only = True)\n",
        "\n",
        "    if runs!=0:\n",
        "        #show_results(off_runs,off_dicts,fs,alphas,parameters)\n",
        "        print_stats(runs,steps)\n",
        "        \n",
        "main()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> f = 1.83, alpha = 0.08 (T2* = 13.00)\n",
            "Particle: [2.5  0.12]\n",
            "[array([2.5 , 0.12])]\n",
            "> Will use 1 datasets for 1 runs each to compute the statistics.\n",
            "> Dataset 0.\n",
            "> Using evenly spaced ordered times.\n",
            "Data vector: [(0.2, 1), (0.2, 0), (0.2648648648648649, 0), (0.2648648648648649, 0), (0.32972972972972975, 0), (0.32972972972972975, 0), (0.3945945945945946, 0), (0.3945945945945946, 1), (0.4594594594594595, 1), (0.4594594594594595, 1), (0.5243243243243243, 1), (0.5243243243243243, 1), (0.5891891891891892, 1), (0.5891891891891892, 1), (0.654054054054054, 0), (0.654054054054054, 0), (0.7189189189189189, 1), (0.7189189189189189, 1), (0.7837837837837838, 0), (0.7837837837837838, 0), (0.8486486486486486, 0), (0.8486486486486486, 0), (0.9135135135135135, 0), (0.9135135135135135, 0), (0.9783783783783784, 1), (0.9783783783783784, 1), (1.0432432432432432, 1), (1.0432432432432432, 1), (1.1081081081081081, 1), (1.1081081081081081, 1), (1.172972972972973, 0), (1.172972972972973, 1), (1.2378378378378379, 1), (1.2378378378378379, 1), (1.3027027027027027, 1), (1.3027027027027027, 0), (1.3675675675675676, 0), (1.3675675675675676, 0), (1.4324324324324325, 0), (1.4324324324324325, 1), (1.4972972972972973, 1), (1.4972972972972973, 0), (1.5621621621621622, 1), (1.5621621621621622, 1), (1.627027027027027, 1), (1.627027027027027, 1), (1.691891891891892, 0), (1.691891891891892, 1), (1.7567567567567568, 1), (1.7567567567567568, 0), (1.8216216216216217, 0), (1.8216216216216217, 1), (1.8864864864864865, 0), (1.8864864864864865, 0), (1.9513513513513514, 0), (1.9513513513513514, 0), (2.0162162162162165, 0), (2.0162162162162165, 1), (2.081081081081081, 1), (2.081081081081081, 1), (2.1459459459459462, 1), (2.1459459459459462, 1), (2.210810810810811, 1), (2.210810810810811, 1), (2.275675675675676, 0), (2.275675675675676, 1), (2.340540540540541, 0), (2.340540540540541, 0), (2.4054054054054057, 0), (2.4054054054054057, 0), (2.4702702702702704, 0), (2.4702702702702704, 0), (2.5351351351351354, 0), (2.5351351351351354, 0), (2.6000000000000005, 0), (2.6000000000000005, 0), (2.664864864864865, 1), (2.664864864864865, 1), (2.72972972972973, 1), (2.72972972972973, 0), (2.794594594594595, 1), (2.794594594594595, 1), (2.85945945945946, 1), (2.85945945945946, 0), (2.9243243243243247, 1), (2.9243243243243247, 0), (2.9891891891891893, 0), (2.9891891891891893, 1), (3.0540540540540544, 1), (3.0540540540540544, 0), (3.1189189189189195, 1), (3.1189189189189195, 1), (3.183783783783784, 1), (3.183783783783784, 1), (3.2486486486486488, 1), (3.2486486486486488, 1), (3.313513513513514, 0), (3.313513513513514, 0), (3.378378378378379, 1), (3.378378378378379, 1), (3.4432432432432436, 1), (3.4432432432432436, 1), (3.5081081081081082, 0), (3.5081081081081082, 0), (3.5729729729729733, 0), (3.5729729729729733, 0), (3.6378378378378384, 0), (3.6378378378378384, 0), (3.702702702702703, 1), (3.702702702702703, 1), (3.7675675675675677, 1), (3.7675675675675677, 1), (3.832432432432433, 1), (3.832432432432433, 0), (3.897297297297298, 1), (3.897297297297298, 1), (3.9621621621621625, 1), (3.9621621621621625, 0), (4.027027027027027, 0), (4.027027027027027, 0), (4.091891891891892, 0), (4.091891891891892, 0), (4.156756756756757, 0), (4.156756756756757, 0), (4.221621621621622, 0), (4.221621621621622, 1), (4.286486486486487, 0), (4.286486486486487, 1), (4.351351351351352, 1), (4.351351351351352, 0), (4.416216216216217, 1), (4.416216216216217, 0), (4.481081081081082, 1), (4.481081081081082, 1), (4.545945945945946, 0), (4.545945945945946, 0), (4.610810810810811, 0), (4.610810810810811, 0), (4.675675675675676, 0), (4.675675675675676, 1), (4.7405405405405405, 1), (4.7405405405405405, 0), (4.805405405405406, 0), (4.805405405405406, 0), (4.870270270270271, 1), (4.870270270270271, 1), (4.935135135135136, 1), (4.935135135135136, 1), (5.0, 1), (5.0, 1)]\n",
            "> t∈[0.2,5.0[; steps = 75; f∈[0.0,5.0[; α∈[0.04,0.20[ (T2*∈[5.00,25.00[)\n",
            "|0%|Performing 10000 Markov move(s) per call. [bayes_update]\n",
            "MH:  Cov =  [[1.0e-03 0.0e+00]\n",
            " [0.0e+00 1.6e-06]]\n",
            "[array([1.85510544, 0.07982143])]\n",
            "33%|[array([1.86742111, 0.09681636])]\n",
            "67%|[array([1.83330791, 0.0694699 ])]\n",
            "100%|\n",
            "> Estimated f=1.83 and alpha=0.07 (T2=14.39).\n",
            "* Number of resampler calls per run: 30000\n",
            "* Metropolis-Hastings:     51% mean particle acceptance rate.\n",
            "(n=1; runs=1; 2d)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}